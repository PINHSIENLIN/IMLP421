{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用Scikit-Learn 完成預測\n",
    "### Scikit-Learn在三個面向提供支援。\n",
    "1. 獲取資料:***klearn.datasets***\n",
    "2. 掌握資料:***sklearn.preprocessing*** \n",
    "3. 機器學習:***sklearn Estimator API*** \n",
    "\n",
    "獲取資料的方式有很多種（包含檔案、資料庫、網路爬蟲、Kaggle Datasets等），<br>\n",
    "其中最簡單的方式是從Sklearn import 內建的資料庫。由於其特性隨手可得且不用下載，所以我們通常叫他**玩具資料**：\n",
    "\n",
    "# 基本架構\n",
    "\n",
    "* 讀取資料&pre-processing\n",
    "* 切分訓練集與測試集 \n",
    "* 模型配適\n",
    "* 預測 \n",
    "* 評估(計算成績可能是誤差值或正確率或..)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 讀取Iris資料集與資料前處理\n",
    "\n",
    "Iris Flowers 資料集\n",
    "\n",
    "我們在這個項目中使用 Iris Data Set，這個資料集中的每個樣本有4個特徵，1個類別。該資料集1中的樣本類別數為3類，每類樣本數目為50個，總共150個樣本。\n",
    "\n",
    "屬性資訊：\n",
    "\n",
    "    花萼長度 sepal length(cm)\n",
    "    花萼寬度 sepal width(cm)\n",
    "    花瓣長度 petal length(cm)\n",
    "    花瓣寬度 petal width(cm)\n",
    "    類別：\n",
    "        Iris Setosa\n",
    "        Iris Versicolour\n",
    "        Iris Virginica\n",
    "\n",
    "樣本特徵資料是數值型的，而且單位都相同（釐米）。\n",
    "\n",
    "![Iris Flowers](images/iris_data.PNG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 印出iris的key值與檔案位置\n",
    "* 查看前10筆資料\n",
    "* 查看資料型別\n",
    "* 印出標註的樣本類別資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.keys())\n",
    "\n",
    "print(iris.filename)\n",
    "\n",
    "print(iris.data[0:10])\n",
    "\n",
    "print(type(iris))\n",
    "\n",
    "print(iris.target)\n",
    "\n",
    "print(iris.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only take the first two features. \n",
    "X = iris.data[:,:2]\n",
    "Y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#以下是組成 pandas DataFrame (也可以不用這種做)\n",
    "x = pd.DataFrame(iris.data, columns = iris.feature_names)\n",
    "x.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"target_names: \" + str(iris.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立Target欄位與資料\n",
    "y = pd.DataFrame(iris.target, columns = [\"target\"])\n",
    "y.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#合併資料特徵欄位與目標欄位\n",
    "iris_data = pd.concat([x,y], axis = 1)[[\"sepal length (cm)\",\"petal length (cm)\",\"target\"]]\n",
    "iris_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#只選擇目標為0與1的資料\n",
    "iris_data = iris_data[iris_data[\"target\"].isin([0,1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 切分訓練集與測試集\n",
    "> train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(iris_data[[\"sepal length (cm)\",\"petal length (cm)\"]], iris_data[\"target\"], test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix \n",
    "\n",
    ">normalization和standardization是差不多的<br>\n",
    "都是把數據進行前處理，從而使數值都落入到統一的數值範圍，從而在建模過程中，各個特徵量沒差別對待。<br> \n",
    "* normalization一般是把數據限定在需要的範圍，比如一般都是【0，1】，從而消除了數據量綱對建模的影響。<br> \n",
    "* standardization 一般是指將數據正態化，使平均值0方差為1.<br> \n",
    "\n",
    "因此normalization和standardization 是針對數據而言的，消除一些數值差異帶來的特種重要性偏見。<br>\n",
    "經過歸一化的數據，能加快訓練速度，促進算法的收斂。\n",
    "\n",
    "### Standardization (z-score)\n",
    "    to compute the mean and standard deviation on a training set so as to be able to later reapply the same transformation on the testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_stats(dfs):\n",
    "    minimum = np.min(dfs)\n",
    "    maximum = np.max(dfs)\n",
    "    mu = np.mean(dfs)\n",
    "    sigma = np.std(dfs)\n",
    "    return (minimum, maximum, mu, sigma)\n",
    "\n",
    "\n",
    "def z_score(col, stats):\n",
    "    m, M, mu, s = stats\n",
    "    df = pd.DataFrame()\n",
    "    for c in col.columns:\n",
    "        df[c] = (col[c]-mu[c])/s[c]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = norm_stats(X_train)\n",
    "arr_x_train = np.array(z_score(X_train, stats))\n",
    "arr_y_train = np.array(y_train)\n",
    "arr_x_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler().fit(X_train)  #Compute the statistics to be used for later scaling.\n",
    "print(sc.mean_)  #mean\n",
    "print(sc.scale_) #standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform: (x-u)/std.\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_train_std[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaler instance can then be used on new data to transform it the same way it did on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_std = sc.transform(X_test)\n",
    "print(X_test_std[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can also use fit_transform method (i.e., fit and then transform)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_std = sc.fit_transform(X_train)  \n",
    "X_test_std = sc.fit_transform(X_test)\n",
    "print(X_test_std[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean of X_train_std:',np.round(X_train_std.mean(),4))\n",
    "print('std of X_train_std:',X_train_std.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-Max Normaliaztion\n",
    "    Transforms features by scaling each feature to a given range.\n",
    "    The transformation is given by:\n",
    "\n",
    "    X' = X - X.min(axis=0) / ((X.max(axis=0) - X.min(axis=0))\n",
    "    X -> N 維資料\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.random.normal(50, 6, 100)  # np.random.normal(mu,sigma,size))\n",
    "y1 = np.random.normal(5, 0.5, 100)\n",
    "\n",
    "x2 = np.random.normal(30,6,100)\n",
    "y2 = np.random.normal(4,0.5,100)\n",
    "plt.scatter(x1,y1,c='b',marker='s',s=20,alpha=0.8)\n",
    "plt.scatter(x2,y2,c='r', marker='^', s=20, alpha=0.8)\n",
    "\n",
    "print(np.sum(x1)/len(x1))\n",
    "print(np.sum(x2)/len(x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = np.concatenate((x1,x2))\n",
    "y_val = np.concatenate((y1,y2))\n",
    "\n",
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_norm(X):\n",
    "    return (X - X.min(axis=0)) / ((X.max(axis=0) - X.min(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_norm(x_val[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "x_val=x_val.reshape(-1, 1)\n",
    "scaler = MinMaxScaler().fit(x_val)  # default range 0~1\n",
    "print(scaler.data_max_)\n",
    "print(scaler.transform(x_val)[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
