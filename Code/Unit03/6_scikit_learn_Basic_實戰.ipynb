{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用Scikit-Learn 完成預測\n",
    "### Scikit-Learn在三個面向提供支援。\n",
    "1. 獲取資料:***klearn.datasets***\n",
    "2. 掌握資料:***sklearn.preprocessing*** \n",
    "3. 機器學習:***sklearn Estimator API*** \n",
    "\n",
    "獲取資料的方式有很多種（包含檔案、資料庫、網路爬蟲、Kaggle Datasets等），<br>\n",
    "其中最簡單的方式是從Sklearn import 內建的資料庫。由於其特性隨手可得且不用下載，所以我們通常叫他**玩具資料**：\n",
    "\n",
    "# 基本架構\n",
    "\n",
    "* 讀取資料&pre-processing\n",
    "* 切分訓練集與測試集 \n",
    "* 模型配適\n",
    "* 預測 \n",
    "* 評估(計算成績可能是誤差值或正確率或..)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 讀取Iris資料集與資料前處理\n",
    "\n",
    "Iris Flowers 資料集\n",
    "\n",
    "我們在這個項目中使用 Iris Data Set，這個資料集中的每個樣本有4個特徵，1個類別。該資料集1中的樣本類別數為3類，每類樣本數目為50個，總共150個樣本。\n",
    "\n",
    "屬性資訊：\n",
    "\n",
    "    花萼長度 sepal length(cm)\n",
    "    花萼寬度 sepal width(cm)\n",
    "    花瓣長度 petal length(cm)\n",
    "    花瓣寬度 petal width(cm)\n",
    "    類別：\n",
    "        Iris Setosa\n",
    "        Iris Versicolour\n",
    "        Iris Virginica\n",
    "\n",
    "樣本特徵資料是數值型的，而且單位都相同（釐米）。\n",
    "\n",
    "![Iris Flowers](images/iris_data.PNG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 150 (50 in each of three classes)\n",
      ":Number of Attributes: 4 numeric, predictive attributes and the class\n",
      ":Attribute Information:\n",
      "    - sepal length in cm\n",
      "    - sepal width in cm\n",
      "    - petal length in cm\n",
      "    - petal width in cm\n",
      "    - class:\n",
      "            - Iris-Setosa\n",
      "            - Iris-Versicolour\n",
      "            - Iris-Virginica\n",
      "\n",
      ":Summary Statistics:\n",
      "\n",
      "============== ==== ==== ======= ===== ====================\n",
      "                Min  Max   Mean    SD   Class Correlation\n",
      "============== ==== ==== ======= ===== ====================\n",
      "sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "============== ==== ==== ======= ===== ====================\n",
      "\n",
      ":Missing Attribute Values: None\n",
      ":Class Distribution: 33.3% for each of 3 classes.\n",
      ":Creator: R.A. Fisher\n",
      ":Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      ":Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. dropdown:: References\n",
      "\n",
      "  - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "    Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "    Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "  - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "    (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "  - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "    Structure and Classification Rule for Recognition in Partially Exposed\n",
      "    Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "    Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "  - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "    on Information Theory, May 1972, 431-433.\n",
      "  - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "    conceptual clustering system finds 3 classes in the data.\n",
      "  - Many, many more ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 印出iris的key值與檔案位置\n",
    "* 查看前10筆資料\n",
    "* 查看資料型別\n",
    "* 印出標註的樣本類別資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n",
      "iris.csv\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]]\n",
      "<class 'sklearn.utils._bunch.Bunch'>\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "print(iris.keys())\n",
    "\n",
    "print(iris.filename)\n",
    "\n",
    "print(iris.data[0:10])\n",
    "\n",
    "print(type(iris))\n",
    "\n",
    "print(iris.target)\n",
    "\n",
    "print(iris.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only take the first two features. \n",
    "X = iris.data[:,:2]\n",
    "Y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#以下是組成 pandas DataFrame (也可以不用這種做)\n",
    "x = pd.DataFrame(iris.data, columns = iris.feature_names)\n",
    "x.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"target_names: \" + str(iris.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立Target欄位與資料\n",
    "y = pd.DataFrame(iris.target, columns = [\"target\"])\n",
    "y.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#合併資料特徵欄位與目標欄位\n",
    "iris_data = pd.concat([x,y], axis = 1)[[\"sepal length (cm)\",\"petal length (cm)\",\"target\"]]\n",
    "iris_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#只選擇目標為0與1的資料\n",
    "iris_data = iris_data[iris_data[\"target\"].isin([0,1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 切分訓練集與測試集\n",
    "> train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(iris_data[[\"sepal length (cm)\",\"petal length (cm)\"]], iris_data[\"target\"], test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix \n",
    "\n",
    ">normalization和standardization是差不多的<br>\n",
    "都是把數據進行前處理，從而使數值都落入到統一的數值範圍，從而在建模過程中，各個特徵量沒差別對待。<br> \n",
    "* normalization一般是把數據限定在需要的範圍，比如一般都是【0，1】，從而消除了數據量綱對建模的影響。<br> \n",
    "* standardization 一般是指將數據正態化，使平均值0方差為1.<br> \n",
    "\n",
    "因此normalization和standardization 是針對數據而言的，消除一些數值差異帶來的特種重要性偏見。<br>\n",
    "經過歸一化的數據，能加快訓練速度，促進算法的收斂。\n",
    "\n",
    "### Standardization (z-score)\n",
    "    to compute the mean and standard deviation on a training set so as to be able to later reapply the same transformation on the testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_stats(dfs):\n",
    "    minimum = np.min(dfs)\n",
    "    maximum = np.max(dfs)\n",
    "    mu = np.mean(dfs)\n",
    "    sigma = np.std(dfs)\n",
    "    return (minimum, maximum, mu, sigma)\n",
    "\n",
    "\n",
    "def z_score(col, stats):\n",
    "    m, M, mu, s = stats\n",
    "    df = pd.DataFrame()\n",
    "    for c in col.columns:\n",
    "        df[c] = (col[c]-mu[c])/s[c]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = norm_stats(X_train)\n",
    "arr_x_train = np.array(z_score(X_train, stats))\n",
    "arr_y_train = np.array(y_train)\n",
    "arr_x_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler().fit(X_train)  #Compute the statistics to be used for later scaling.\n",
    "print(sc.mean_)  #mean\n",
    "print(sc.scale_) #standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform: (x-u)/std.\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_train_std[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaler instance can then be used on new data to transform it the same way it did on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_std = sc.transform(X_test)\n",
    "print(X_test_std[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can also use fit_transform method (i.e., fit and then transform)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_std = sc.fit_transform(X_train)  \n",
    "X_test_std = sc.fit_transform(X_test)\n",
    "print(X_test_std[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean of X_train_std:',np.round(X_train_std.mean(),4))\n",
    "print('std of X_train_std:',X_train_std.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-Max Normaliaztion\n",
    "    Transforms features by scaling each feature to a given range.\n",
    "    The transformation is given by:\n",
    "\n",
    "    X' = X - X.min(axis=0) / ((X.max(axis=0) - X.min(axis=0))\n",
    "    X -> N 維資料\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.random.normal(50, 6, 100)  # np.random.normal(mu,sigma,size))\n",
    "y1 = np.random.normal(5, 0.5, 100)\n",
    "\n",
    "x2 = np.random.normal(30,6,100)\n",
    "y2 = np.random.normal(4,0.5,100)\n",
    "plt.scatter(x1,y1,c='b',marker='s',s=20,alpha=0.8)\n",
    "plt.scatter(x2,y2,c='r', marker='^', s=20, alpha=0.8)\n",
    "\n",
    "print(np.sum(x1)/len(x1))\n",
    "print(np.sum(x2)/len(x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = np.concatenate((x1,x2))\n",
    "y_val = np.concatenate((y1,y2))\n",
    "\n",
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_norm(X):\n",
    "    return (X - X.min(axis=0)) / ((X.max(axis=0) - X.min(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_norm(x_val[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "x_val=x_val.reshape(-1, 1)\n",
    "scaler = MinMaxScaler().fit(x_val)  # default range 0~1\n",
    "print(scaler.data_max_)\n",
    "print(scaler.transform(x_val)[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
